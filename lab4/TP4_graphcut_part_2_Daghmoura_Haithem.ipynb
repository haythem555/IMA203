{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_8tPk8fYgZa"
   },
   "source": [
    "# Practical work on graph-cut optimization (part 2, multilevel)\n",
    "\n",
    "The objective of this PW is the implementation of the $\\alpha$-expansion and $\\alpha$-$\\beta$ swap approaches for grayscale image denoising.\n",
    "\n",
    "The PyMaxFlow library is used to compute the graph-cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y-4ipq0XGDm"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "import tempfile\n",
    "import os\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "!pip install PyMaxflow==1.2.13\n",
    "import maxflow\n",
    "\n",
    "from skimage import io\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.plotting import show as showbokeh\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def affiche_pour_colab(im,MINI=None, MAXI=None,titre=''): #special colab, don't look\n",
    "  def normalise_image_pour_bokeh(X,MINI,MAXI):\n",
    "    if MAXI==None:\n",
    "      MAXI = np.max(X)\n",
    "    if MINI==None:\n",
    "      MINI = np.min(X)\n",
    "    imt=np.copy(X.copy())\n",
    "    imt=(np.clip(imt,MINI,MAXI)/(MAXI-MINI))\n",
    "    imt[imt<0]=0\n",
    "    imt[imt>1]=1\n",
    "    imt*=255\n",
    "    sortie=np.empty((*imt.shape,4),dtype=np.uint8)\n",
    "    for k in range(3):\n",
    "      sortie[:,:,k]=imt\n",
    "    sortie[:,:,3]=255\n",
    "    return sortie\n",
    "\n",
    "  img = im\n",
    "  img=normalise_image_pour_bokeh(np.flipud(im),MINI,MAXI)\n",
    "  p = figure(tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")], y_range=[im.shape[0], 0], x_range=[0, im.shape[1]], title=titre)\n",
    "  # p.x_range.range_padding = p.y_range.range_padding = 0\n",
    "  # must give a vector of images\n",
    "  p.image(image=[im], x=0, y=0, dw=im.shape[1], dh=im.shape[0], palette=\"Greys256\", level=\"image\")\n",
    "  p.xgrid.visible = False\n",
    "  p.ygrid.visible = False\n",
    "  showbokeh(p)\n",
    "\n",
    "def affiche(im,MINI=0.0, MAXI=None,titre='',printname=False):\n",
    "  affiche_pour_colab(im,MINI=MINI, MAXI=MAXI,titre=titre) # under google colab many options disappear\n",
    "\n",
    "def display_segmentation_borders(image, bin):\n",
    "    imagergb = np.copy(image)\n",
    "    from skimage.morphology import binary_dilation, disk\n",
    "    contour = binary_dilation(bin,disk(15))^bin\n",
    "    imagergb[contour==1,0] = 255\n",
    "    imagergb[contour==1,1] = 0\n",
    "    imagergb[contour==1,2] = 0\n",
    "    return imagergb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuvZnA9KXGER"
   },
   "source": [
    "## 2 Denoising a grayscale image\n",
    "\n",
    "\n",
    "In this second part of the PW, we are interested in using Markovian methods to **denoise** images with different regularization potentials.\n",
    "\n",
    "We are interested in denoising the images *Ibruitee.png* and *Ibruitee2.png* which correspond to the same scene perturbed by two noises of different nature.\n",
    "\n",
    "\n",
    "We will complete programs that call the algorithm of alpha-expansions or Boykov's alpha-beta swap according to the Kolmogorov technique.\n",
    "\n",
    "\n",
    "Q1: What are the respective expressions for the data attachment potentials in the case of noise following a Gaussian distribution (equation 1) and a Rayleigh distribution (equation 2)?\n",
    "\n",
    "\\begin{equation}\n",
    "p(y_p|x_p)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(y_p-x_p)^2}{2\\sigma^2}\\right],\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(y_p|x_p)=2\\frac{y_p}{x_p^2}\\exp\\left[-\\frac{y_p^2}{x_p^2}\\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RufJ1sxBXGET"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A1:\n",
    "the data attachement term for the case of gaussian distribution $U_{att} = -ln(P(y_p|x_p) = -ln(\\sqrt{2\\pi σ}) + \\frac{(y_p-x_p)^2}{2 σ} $  and we can redefine it as : $U_{att}= (y_p-x_p)^2 $\n",
    "\n",
    "In the case of the Rayleigh distribution we get : $U_{att} = -ln(2)-ln(y_s)+2ln(x_p)+\\frac{y_p^2}{x_p^2}$ we can redfine it as : $U_{att} = -ln(y_s)+2ln(x_p)+\\frac{y_p^2}{x_p^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIXK16BxXGEU"
   },
   "source": [
    "Q2: By studying the histogram of a homogeneous area, indicate which type of noise is present in which image.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mLHdc7RXGEU"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A2: In the first image we noitice that we have a gaussian noise. In the second image we have a Rayleigh noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "pdgSPkwMbipt",
    "outputId": "59878f79-b396-4828-b0c3-f8a701d720d5"
   },
   "outputs": [],
   "source": [
    "im_obs=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/Ibruitee.png') # Observed image, noisy\n",
    "im_obs2=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/Ibruitee2.png') # Observed image, noisy\n",
    "im_orig=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/IoriginaleBW.png') # Reference binary image, to evaluate the quality of the segmentation\n",
    "\n",
    "I = im_obs\n",
    "affiche(im_obs,MINI=0.0, MAXI=255.,titre=\"Noisy image\",printname=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "py2uiyrVXGEV",
    "outputId": "52baeefc-2913-4f2c-d622-6f3d4f7b1e27"
   },
   "outputs": [],
   "source": [
    "# select a homogeneous area to study the histogram\n",
    "# we can use the area from 45 to 120 and from 175 to 245\n",
    "affiche(im_obs[:,:],MINI=0.0, MAXI=255.,titre=\"Noisy image\",printname=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(im_obs[45:120,175:245].flatten(),50)\n",
    "plt.title('Dark area image 1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(im_obs2[45:120,175:245].flatten(),50)\n",
    "plt.title('Dark area image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_URNoqaXGEX"
   },
   "source": [
    "\n",
    "We will compare three *a priori* models: the Potts model $\\delta_{x_p=x_q}$, the (discrete) total variation $|x_p-x_q|$, and the (quadratic) Gaussian model $(x_p-x_q)^2$.\n",
    "\n",
    "Q3: Are they metrics or semi-metrics? What can we deduce about the optimization method to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF1p0Iz7XGEX"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A3:\n",
    "In order to know wether a model is (semi) metric or not we need to verify the following properties on  the potentiel expression :\n",
    "\n",
    "Semi -metric if $\\forall \\alpha ,\\beta \\in E^2 :$\n",
    "\n",
    "- $V_c(\\alpha ,\\beta) =V_c(\\beta , \\alpha) \\geq 0$\n",
    "\n",
    "- $V_c(\\alpha ,\\beta) = 0 ⇔ \\alpha = \\beta$\n",
    "\n",
    "Metric if we have additianlly  :     \n",
    "\n",
    "- $V_c(\\alpha,\\beta) \\leq V_c(\\gamma,\\beta) + V_c(γ,\\beta) $\n",
    "\n",
    "\n",
    "We conclude :     \n",
    "\n",
    "\n",
    "-Potts is metric\n",
    "\n",
    "-Quadratic is semi-metric\n",
    "\n",
    "-total variation is metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkpjifIHXGEY"
   },
   "source": [
    "Q4: What are the differences between the alpha expansion method and the alpha-beta swap method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FhVeTAWXGEY"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A4:\n",
    "\n",
    "In the alpha-beta swap we are comparing between two classes at a time. We face each pixel to a choice between keeping a current value or swapping it with the other (alpha or beta). However, in the the alpha expansion we are dividing the set of labels into two disjoint sets : $\\alpha$ and $\\bar{\\alpha} = L∖\\alpha$ and we are deciding wether to keep the current pixel label or to give it the value $\\alpha$. An other important difference is that the $\\alpha - \\beta $ swap works on the semi metric régularisation expression but the $\\alpha$ expension works only on metrics.\n",
    "\n",
    "\n",
    "There are other implementation details that differs as well like the importance of the intialization for the case of the $\\alpha - \\beta $ swap and the graph representation in each case : for the $\\alpha - \\beta $ swap we represent a subset of the pixels but in the $\\alpha $ expension we need to represent all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEvPlwtMMewg"
   },
   "source": [
    "In the following sections we will use the functions aexpansion_grid and abswap_grid which perform the alpha-expansion and alpha-beta swap respectively. These functions take as input two arguments for a number of levels L :\n",
    "- a tensor of the image size containing in the 3rd dimension the data attachment of each pixel for each considered level (unary term)\n",
    "- a matrix containing the values of the interaction terms between two levels $l_1$ and $l_2$ (depends on the chosen interaction potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0HKVijFXGEZ"
   },
   "source": [
    "### 2.1 Denoising in the Gaussian case (synthetic image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZj8fxA4XGEZ",
    "outputId": "fdc8e94f-f380-4be6-afdd-e7345f6042b3"
   },
   "outputs": [],
   "source": [
    "from maxflow.fastmin import aexpansion_grid, abswap_grid\n",
    "\n",
    "# Loading images\n",
    "im_obs=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/Ibruitee.png').astype('float') # Observed image, noisy\n",
    "im_obs2=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/Ibruitee2.png').astype('float') # Observed image, noisy\n",
    "im_orig=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/IoriginaleBW.png').astype('float') # Reference binary image, to evaluate the quality of the segmentation\n",
    "\n",
    "I = im_obs\n",
    "\n",
    "I = I*255/I.max()\n",
    "L = 30\n",
    "# Generates L gray levels for nearsest prototype labeling\n",
    "levs = np.arange(0, 255, 255/L)\n",
    "# Calculate data cost as the absolute difference between the label prototype and the pixel value\n",
    "D = np.double(np.abs((I.reshape(I.shape+(1,)) - levs.reshape((1,1,-1)))**2))\n",
    "\n",
    "affiche(I,MINI=0.0, MAXI=255.,titre=\"Noisy image\",printname=True)\n",
    "\n",
    "# Generate nearest prototype labeling\n",
    "Id = np.argmin(D,2)\n",
    "affiche(Id*255/L,MINI=0.0, MAXI=255.,titre=\"Maximum likelihood denoising\",printname=True)\n",
    "print(D.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JLXkTAiXGEa"
   },
   "source": [
    "Q5: Here, what does the array D correspond to? Explain its dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_JfRGJiXGEb"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A5: The array D contains the data attachement term for the gaussian model for each pixel ( hence the size of the image 323 X 261) and for each gray level (30 : the number of levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NKHbfwgXGEd"
   },
   "source": [
    "Complete the programs below, and test each regularization model by determining an appropriate beta value each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWez9gd2XGEf",
    "outputId": "138addf9-b683-4e04-c58f-11561cbcabe5"
   },
   "outputs": [],
   "source": [
    "# Potts model regularization\n",
    "# beta values of several hundred\n",
    "beta_Potts =2000\n",
    "# definition of the regularization matrix V(i,j) for the Potts model\n",
    "V_Potts = np.double(beta_Potts*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))>0))\n",
    "affiche(V_Potts)\n",
    "\n",
    "# Performs the alpha-expansion based on the data attachment D and the regularization V\n",
    "labels_Potts = aexpansion_grid(D,V_Potts)\n",
    "affiche(labels_Potts*255/L,MINI=0.0, MAXI=255.0,titre=\"Graph Cut Denoising, Potts regularization, beta = \" + str(beta_Potts),printname=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRl4Tu3CXGEg",
    "outputId": "4cc94dcd-8d2e-4562-e7e5-f1f9c89c761d"
   },
   "outputs": [],
   "source": [
    "# TV model regularization\n",
    "# beta value of the order of tens\n",
    "beta_TV = 40\n",
    "# definition of the regularization matrix V(i,j) for the TV model\n",
    "V_TV = np.double(beta_TV*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))))\n",
    "affiche(V_TV)\n",
    "\n",
    "# Performs the alpha-expansion based on the data attachment D and the regularization V\n",
    "labels_TV = aexpansion_grid(D,V_TV)\n",
    "print(\"Calcul TV terminé\")\n",
    "affiche(labels_TV*255/L,MINI=0.0, MAXI=255.0,titre=\"Graph Cut Denoising, TV regularization, beta = \" + str(beta_TV),printname=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJRIlf5bbT36"
   },
   "source": [
    "For the quadratic regularization model we can no longer use the $\\alpha$ expension method since it is not a metric. So for this case we will use the $\\alpha - \\beta$ swap algorithm that supports the semi-metric models like the quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScWpsJscXGEg",
    "outputId": "0218175b-f2cb-46a3-d29c-39f50c05f55a"
   },
   "outputs": [],
   "source": [
    "# Quadratic model regularization\n",
    "# beta value of the order of tens\n",
    "beta_quadratic = 6\n",
    "# definition of the regularization matrix V(i,j) for the quadratic model\n",
    "V_quadratic =np.double(beta_quadratic*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1))))**2)\n",
    "affiche(V_quadratic)\n",
    "\n",
    "# Performs the alpha-beta swap based on the data attachment D and the regularization V\n",
    "# The abswap_grid contains a no longer supported syntax of slicing and thus the current implementation will fail. To solve this I made some changes in the\n",
    "# the maxflow.fastmin.py file and corrected it . In order to use it you need to download that file and put it in the same directory as this notebook\n",
    "labels_Quadratic = abswap_grid(D,V_quadratic)\n",
    "affiche(labels_Quadratic*255/L,MINI=0.0, MAXI=255,titre=\"Graph Cut Denoising, TV regularization, beta = \" + str(beta_quadratic),printname=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOIjm_GvXGEj"
   },
   "source": [
    "Q6: Which regularization model do you think is best? Give the best regularization parameter visually found and comment on the results you get in each of the three cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mXb0h35XGEj"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A6: For this simple image, the potts model gives us the best results. The best parameters obtained are 2000,40,4.\n",
    "The potts results gives are quite accurate: It is able to identify connex regions and doesn't mix it with others. This is espacially clear in the case of the background or the big black rectangles. This is not the case for the other two models where we are getting mixed values for the same homogenous regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E03KjnuhXGEk"
   },
   "source": [
    "### 2.2 Denoising in the case of speckle noise (synthetic image)\n",
    "\n",
    "Modify the following cells to fit the model for denoising the im_obs2 image.\n",
    "\n",
    "Q7: Which modifications are needed? (There are several!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vig5FFxSXGEk"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A7: The data attachement term has to change since we no longer have the gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykgbMadRXGEl",
    "outputId": "a668e085-b1a3-4af2-9626-3aad580e2077"
   },
   "outputs": [],
   "source": [
    "from maxflow.fastmin import aexpansion_grid\n",
    "from numpy import log\n",
    "\n",
    "# Loading image\n",
    "im_obs2=io.imread('https://perso.telecom-paristech.fr/tupin/TPGRAPHCUT/OLD/Ibruitee2.png').astype('float') # Observed image, noisy\n",
    "I = im_obs2\n",
    "I = I*255/I.max()\n",
    "affiche(I,MINI=0.0, MAXI=255.,titre=\"Noisy image 2\",printname=True)\n",
    "\n",
    "L = 30\n",
    "# Generates L gray levels for nearsest prototype labeling\n",
    "levs = np.arange(1/L, 255, 255/L)\n",
    "\n",
    "# Calculate data cost as the absolute difference between the label prototype and the pixel value\n",
    "D = -log(2*I.reshape(I.shape+(1,))/levs.reshape((1,1,-1))**2)+(I.reshape(I.shape+(1,))**2/levs.reshape((1,1,-1))**2)\n",
    "print()\n",
    "Id = np.argmin(D,2)\n",
    "\n",
    "# Generate nearest prototype labeling\n",
    "Id = np.argmin(D,2)\n",
    "affiche(Id/L*255,MINI=0.0, MAXI=255.,titre=\"Maximum likelihood denoising\",printname=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_sNG98TXGEm",
    "outputId": "67998e3a-5af9-4acb-d57c-aec350664204"
   },
   "outputs": [],
   "source": [
    "# beta value in the order of tenths of a unit\n",
    "beta_Potts = 0.5\n",
    "# definition of the Potts potential matrix\n",
    "V_Potts = np.double(beta_Potts*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))>0))\n",
    "\n",
    "# Performs the alpha-expansion based on the data attachment D and the regularization V\n",
    "labels_Potts = aexpansion_grid(D,V_Potts)\n",
    "affiche(labels_Potts*255/L,MINI=0.0, MAXI=255,titre=\"Graph Cut Denoising, Potts regularization, beta = \" + str(beta_Potts),printname=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6D46cNQWXGEn",
    "outputId": "ba348e63-6606-4eac-a4bc-1d1132b065fd"
   },
   "outputs": [],
   "source": [
    "# beta value in hundredths of a unit\n",
    "beta_TV =0.02\n",
    "# definition of the regularization matrix for TV\n",
    "V_TV =np.double(beta_TV*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))))\n",
    "\n",
    "# Performs the alpha-expansion based on the data attachment D and the regularization V\n",
    "labels_TV =aexpansion_grid(D,V_TV)\n",
    "print(\"TV computation completed\")\n",
    "affiche(labels_TV*255/L,MINI=0.0, MAXI=255,titre=\"Graph Cut Denoising, TV regularization, beta = \" + str(beta_TV),printname=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhuE-M1Pe69R"
   },
   "source": [
    "Same, here we use the $\\alpha - \\beta$ swap algo for the quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHoMB9AIXGEn",
    "outputId": "33ba3423-6490-467f-bfd0-072f1612b25c"
   },
   "outputs": [],
   "source": [
    "# beta value in the order of thousandths of a unit\n",
    "beta_quadratic = 0.001\n",
    "# definition of the regularization matrix for a quadratic potential\n",
    "V_quadratic =np.double(beta_quadratic*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1))))**2)\n",
    "\n",
    "# Performs the alpha-expansion based on the data attachment D and the regularization V\n",
    "labels_Quadratic = abswap_grid(D,V_quadratic)\n",
    "affiche(labels_Quadratic*255/L,MINI=0.0, MAXI=255.,titre=\"Graph Cut Denoising, Quadratic regularization, beta = \" + str(beta_quadratic),printname=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNwTqDmmXGEo"
   },
   "source": [
    "### 2.3 Denoising a natural image\n",
    "\n",
    "Apply one of the methods used above to denoise the noisy cameraman image. Justify your choice.\n",
    "\n",
    "Q8: Comment on the result obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BYnMvduXGEo"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A8: It is more intressting to use the TV model since this image isn't constant piecewise like the pervious example. In addition the TV model is a metric and thus this will allow us to use the $\\alpha$ expansion algorithm which is better than the $\\alpha - \\beta$ swap\n",
    "\n",
    "We can say that for the chosen value of beta, the result was average. We succeded in restoring the objects' contours of the image like the man and the camera, however we couldn't restore the the building in the background. This is due to the high level of the noise present in the image (high noise to signal ratio). Thus, such a result was expected from the begining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DAAdzQksXGEq",
    "outputId": "a7776022-4258-40a1-d8f9-ba6fcb1b98a2"
   },
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "I_cameraman = imageio.imread('https://people.math.sc.edu/Burkardt/data/tif/cameraman.tif')\n",
    "I_cameraman_bruit = I_cameraman + np.random.normal(0,80,I_cameraman.shape)\n",
    "I_cameraman_bruit[I_cameraman_bruit<0] = 0\n",
    "I_cameraman_bruit[I_cameraman_bruit>255] = 255\n",
    "print(I_cameraman.max())\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(I_cameraman, cmap='gray')\n",
    "plt.title('Image without noise')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(I_cameraman_bruit, cmap='gray')\n",
    "plt.title('Noisy image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lThTq8-vXGEq",
    "outputId": "0c4c3c96-70db-4dac-9102-d3ff7fc0ac69"
   },
   "outputs": [],
   "source": [
    "I = 255*I_cameraman_bruit/I.max()\n",
    "L = 250\n",
    "# Generates L gray levels for nearsest prototype labeling\n",
    "levs =np.arange(0,255,255/L)\n",
    "# Calculate data cost as the absolute difference between the label prototype and the pixel value\n",
    "D =np.double(np.abs((I.reshape(I.shape+(1,))- levs.reshape((1,1,-1)))**2))\n",
    "\n",
    "# choose a regularization model and compute V matrix\n",
    "Id= np.argmin(D,2)\n",
    "affiche(Id, MINI=0.0,MAXI=None, titre=\"Maximum likelihood denoising\",printname=True)\n",
    "beta_TV= 120\n",
    "V_TV = np.double(beta_TV*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))))\n",
    "\n",
    "# compute the appropriate optimization\n",
    "labels_TV=aexpansion_grid(D,V_TV)\n",
    "# display the regularized image\n",
    "affiche(labels_TV, MINI=0.0,MAXI=None,titre=\"Graph Cut Denoising, TV, beta = \" + str(beta_TV),printname=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAFknqfAXGEu"
   },
   "source": [
    "### 2.3 SAR Image Denoising\n",
    "\n",
    "SAR (Synthetic Aperture Radar) imagery is a radar-based remote sensing modality that provides images of the Earth in all light and weather conditions. A major drawback is the high speckle noise that affects them. The following cell loads an amplitude image acquired by the Sentinel 1-A satellite over the city of Des Moines in the USA. To limit the computation time, we will work on a small rectangle from the image provided.\n",
    "\n",
    "Adapt one of the methods used previously to denoise the image provided. We will assume that the noise follows a Rayleigh distribution.\n",
    "\n",
    "Q9: Comment on the result obtained.\n",
    "\n",
    "We can compare the result with a denoising obtained by a more recent method (SAR2SAR), based on a Deep Learning approach (the code to display it is provided below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__5ab5afUQGv"
   },
   "source": [
    "**Your answer &#x270D;**\n",
    "\n",
    "A9:\n",
    "We have obtained a relatively good result using the TV model for this chosen beta value, taking into account the high quantity of noise. We have obbtained an acceptable denoised image, however, we have lost certain points of the river contour. The results obtained by the SAR2SAR methode are far superior. Additionaly, we have been able to completly denoise the image in a relatively simlar execution time to that of the alpha expansion for this little part of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vK1DPdHkXGEw",
    "outputId": "5b8ca281-0a42-47c4-b9f9-cc2e294a1a33"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    I_SAR = np.load('noisy_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl=1')\n",
    "except:\n",
    "    !wget 'https://www.dropbox.com/s/7m2dw3irho8dpzj/noisy_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl='\n",
    "    I_SAR = np.load('noisy_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl=1')\n",
    "\n",
    "affiche(I_SAR,0,100,'Full image')\n",
    "I_SAR = I_SAR[0:300,0:300]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(I_SAR,vmax = 100, cmap = 'gray') # The display is done by truncating the dynamic\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(I_SAR[100:200,100:200].flatten(),200) # Display of the histogram on an almost homogeneous area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kaj9Y2rFXGEx",
    "outputId": "e1b3b358-a7ae-41f9-835f-4e989d792ccd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "I = I_SAR*255/I_SAR.max()\n",
    "L = 255\n",
    "# Generates L gray levels for nearsest prototype labeling\n",
    "levs = np.arange(1/L,255,255/L)\n",
    "# Calculate data cost as the neg-log likelihood\n",
    "D = -2*log(I.reshape(I.shape+(1,))/levs.reshape((1,1,-1))**2)+(I.reshape(I.shape+(1,))**2/levs.reshape((1,1,-1))**2)\n",
    "print()\n",
    "affiche(I,MINI=0.0, MAXI=None,titre=\"Noisy Image\",printname=True)\n",
    "\n",
    "# choose a regularization model and compute V matrix\n",
    "Id= np.argmin(D,2)\n",
    "affiche(Id, MINI=0.0,MAXI=None, titre=\"Maximum likelihood denoising\",printname=True)\n",
    "\n",
    "beta_TV= 0.15\n",
    "V_TV = np.double(beta_TV*(np.abs( levs.reshape((-1,1)) - levs.reshape((1,-1)))))\n",
    "\n",
    "# compute the appropriate optimization\n",
    "labels_TV=aexpansion_grid(D,V_TV)\n",
    "\n",
    "\n",
    "# display the regularized image\n",
    "affiche(labels_TV, MINI=0.0,MAXI=None,titre=\"Graph Cut Denoising, regularisation TV , beta = \" + str(beta_TV),printname=True)\n",
    "plt.figure()\n",
    "plt.imshow(labels_TV,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oJD1bXP-CXPy",
    "outputId": "ca1fdb39-7d5e-411a-8329-61f6e659c3ad"
   },
   "outputs": [],
   "source": [
    "# Display of the denoised image by the SAR2SAR method\n",
    "\n",
    "try:\n",
    "    I_SAR = np.load('denoised_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl=1')\n",
    "except:\n",
    "    !wget 'https://www.dropbox.com/s/0f6l0qr6teck5bd/denoised_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl=1'\n",
    "    I_SAR = np.load('denoised_DesMoines_dual_1_corrige_1_VV_AMPLITUDE.npy?dl=1')\n",
    "\n",
    "affiche(I_SAR,0,100,'Full image')\n",
    "I_SAR = I_SAR[0:300,0:300]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(I_SAR,vmax = 100, cmap = 'gray') # The display is done by truncating the dynamic\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(I_SAR[100:200,100:200].flatten(),200) # Display of the histogram on an almost homogeneous area\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z0HKVijFXGEZ",
    "E03KjnuhXGEk"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
